import time
import re
import os
import random
import math
from collections import defaultdict


class Ngram_Language_Model:
    """The class implements a Markov language model (LM) that learns a LM from a given text.
        It supoprts language generation and the evaluation of a given string.
        The class can be applied on both word level and character level.
    """

    def __init__(self, n=3, chars=False):
        """Initializing a LM object.

        Args:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True if the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """
        self.n = n
        self.chars = chars
        self.model_dict = defaultdict(int)  # dictionary {ngram:count} holding ngrams counts
        self.model_dict_prefix = defaultdict(int)  # dictionary {prefix:count} holding prefixes ((n-1)grams) counts
        if n > 2:
            self.model_dict_bigram = defaultdict(int)  # a dictionary of the form {bigram:count}, holding bigrams counts
        else:
            self.model_dict_bigram = None
        self.vocab = defaultdict(int)
        self.vocab_size = None  # number of total words (without repetition) in vocab
        self.vocab_count = None  # number of total words (with repetitions) in vocab

    def build_model(self, text):
        """populates the instance variable model_dict.

            Args:
                text (str): the text to construct the model from.
        """

        if self.chars:  # for character_level model - create a list of all characters (as grams for the model)
            grams_list = [c for c in text]
        else:
            grams_list = text.split()  # for word_level model - create list of all words (as grams for the model)
        n = self.n

        # populating self.model_dict which counts ngrams in the text
        for i in range(len(grams_list) - n + 1):
            if self.chars:
                ngram = ''.join(grams_list[i: i + n])
            else:
                ngram = ' '.join(grams_list[i: i + n])
            if self.model_dict.get(ngram):
                self.model_dict[ngram] += 1
            else:
                self.model_dict[ngram] = 1

        # populating self.model_dict_prefix which counts (n-1grams) in the text
        for i in range(len(grams_list) - (n - 1) + 1):
            if self.chars:
                prefix = ''.join(grams_list[i: i + (n - 1)])
            else:
                prefix = ' '.join(grams_list[i: i + (n - 1)])
            if self.model_dict_prefix.get(prefix):
                self.model_dict_prefix[prefix] += 1
            else:
                self.model_dict_prefix[prefix] = 1

        # populating self.model_dict_bigram in the text
        if self.model_dict_bigram is not None:
            for i in range(len(grams_list) - (n - 1) + 1):
                if self.chars:
                    bigram = ''.join(grams_list[i: i + 2])
                else:
                    bigram = ' '.join(grams_list[i: i + 2])
                if self.model_dict_bigram.get(bigram):
                    self.model_dict_bigram[bigram] += 1
                else:
                    self.model_dict_bigram[bigram] = 1

        # populating self.vocab which counts words in the text
        for word in grams_list:
            if self.vocab.get(word):
                self.vocab[word] += 1
            else:
                self.vocab[word] = 1

        self.vocab_size = len(self.vocab)
        self.vocab_count = len([w_count for w_count in self.vocab.values()])

    def get_model_dictionary(self):
        """Returns the dictionary class object
        """
        return self.model_dict

    def get_model_window_size(self):
        """Returning the size of the context window (the n in "n-gram")
        """
        return self.n

    def generate(self, context=None, n=20):
        """Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context is sampled
        from the models' contexts distribution. Generation is stopped before the n'th word if the
        contexts are exhausted.

            Args:
                context (str): a seed context to start the generated string from.
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

        """
        convert_eos_sos_to_dots = False  # modify to True in case user wish to output ' . ' instead of 'sos' (start-of-sentence) and 'eos' (end-of-sentence) tokens
        deterministic = False  # modify to True in case user wish to select the words for text generation deterministically
        gen_not_possible = False
        complete_with_unigrams = False

        if not context:
            context = random.choices(list(self.model_dict.keys()), weights=list(self.model_dict.values()))[0]

        if self.chars:
            context_grams = [c for c in context]
        else:
            context_grams = context.split()  # [- self.n + 1:]

        if len(context_grams) >= n:
            raise Exception(f"Number of context-words ({len(context_grams)}) is higher then 'n' ({n})")

        generated_text = context_grams.copy()

        if len(context_grams) >= self.n:
            context_grams = context_grams[- self.n + 1:]

        for i, gram in enumerate(context_grams):
            if not self.vocab.get(gram) and i == len(context_grams) - 1:
                gen_not_possible = True
                missing_gram = gram
                break
            elif not self.vocab.get(gram):
                complete_with_unigrams = True
                complete_x_grams = i + 1
                context_grams = context_grams[i + 1:]
                missing_gram = gram
                break

        if gen_not_possible:
            raise Exception(
                f"Last gram in context ('{context_grams[-1]}') is not in vocabulary; not possible to generate")

        if complete_with_unigrams:
            for i in range(complete_x_grams):
                unigram = context_grams[-1]
                unigram_counts = self.vocab[unigram]
                relative_freq = dict()
                for gram in self.vocab.keys():
                    if self.chars:
                        bigram = ''.join([unigram, gram])
                    else:
                        bigram = ' '.join([unigram, gram])
                    bigram_counts = self.model_dict_bigram[bigram]
                    relative_freq[gram] = bigram_counts / unigram_counts

                if not sum([v for v in relative_freq.values()]):
                    raise Exception(
                        f"Can't complete generation; '{missing_gram}' is not in vocabulary, and can't find any bigrams which starts with '{unigram}' unigram")

                # deterministic choice is set to False by default, in order to allow various options during text generation
                if deterministic:
                    selected_gram = max(relative_freq, key=relative_freq.get)
                else:
                    selected_gram = random.choices(list(relative_freq.keys()), weights=(list(relative_freq.values())))[
                        0]
                context_grams.append(selected_gram)

        for i in range(n - len(generated_text)):
            relative_freq = dict()
            for word in self.vocab.keys():
                # creating ngram/prefix differently based on char/word level model
                if self.chars:
                    prefix = ''.join(context_grams)
                    ngram = ''.join([prefix, word])
                else:
                    prefix = ' '.join(context_grams)
                    ngram = ' '.join([prefix, word])

                if self.model_dict.get(ngram) and self.model_dict_prefix.get(prefix):
                    ngram_counts = self.model_dict[ngram]
                    prefix_counts = self.model_dict_prefix[prefix]
                    relative_freq[word] = ngram_counts / prefix_counts
                else:
                    relative_freq[word] = 0

            # deterministic choice is set to False by default, in order to allow various options during text generation
            if deterministic:
                selected_word = max(relative_freq, key=relative_freq.get)
            else:
                selected_word = random.choices(list(relative_freq.keys()), weights=(list(relative_freq.values())))[0]
            context_grams.append(selected_word)
            context_grams = context_grams[1:]
            generated_text.append(selected_word)

        if self.chars:
            generated_text = ''.join(generated_text)
        else:
            generated_text = ' '.join(generated_text)

        # removing <eos> / <sos> tokens in case using them in normalized text
        if convert_eos_sos_to_dots:
            generated_text = re.sub(r"\s*<eos>\s*<sos>\s*", " . ", generated_text)

        return generated_text

    def evaluate(self, text):
        """Returns the log-likelihood of the specified text to be a product of the model.
           Laplace smoothing is applied if necessary.

           Args:
               text (str): Text to evaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        smoothing = False
        log_likelihood = 0
        text = normalize_text(text).strip()

        # in case text is shorter than lm.window_size
        if self.chars and len(text) < self.n:
            raise Exception(
                f"evaluation is not possible for char-level-model with # chars ('{len(text)}') < 'n' ('{self.n}')")
        elif len(text.split()) < self.n:
            if len(text.split()) == 1:
                gram_counts = self.vocab.get(text, 0)
                if gram_counts:
                    log_likelihood = math.log(gram_counts / self.vocab_count)
                    return log_likelihood
                else:
                    raise Exception(f"word ('{text}') to evaluate doesn't exist in corpus.")
            else:
                text_bigrams = text.split()
                text_bigrams = [' '.join(text_bigrams[idx: idx + 2]) for idx, w in enumerate(text_bigrams[:-1])]
                for bigram in text_bigrams:
                    if not self.model_dict_bigram.get(bigram):
                        smoothing = True

                if smoothing:
                    for bigram in text_bigrams:
                        unigram = bigram.split()[0]
                        unigram_counts = self.vocab.get(unigram, 0)
                        bigram_counts = self.model_dict_bigram.get(bigram, 0) + 1
                        ll_ngram = math.log(bigram_counts / (unigram_counts + self.vocab_size))
                        log_likelihood += ll_ngram
                else:
                    for bigram in text_bigrams:
                        unigram = bigram.split()[0]
                        unigram_counts = self.vocab.get(unigram, 0)
                        bigram_counts = self.model_dict_bigram.get(bigram, 0)
                        ll_ngram = math.log(bigram_counts / unigram_counts)
                        log_likelihood += ll_ngram

                return log_likelihood

        if self.chars:
            text_grams = text
            text_ngrams = [''.join(text_grams[idx: idx + self.n]) for idx, w in enumerate(text_grams[:-self.n + 1])]
        else:
            text_grams = text.split()
            text_ngrams = [' '.join(text_grams[idx: idx + self.n]) for idx, w in enumerate(text_grams[:-self.n + 1])]

        for ngram in text_ngrams:
            if not self.model_dict.get(ngram, 0):
                smoothing = True
                break

        for ngram in text_ngrams:
            if self.chars:
                prefix = ''.join(ngram.split()[:-1])
            else:
                prefix = ' '.join(ngram.split()[:-1])

            if smoothing:
                ll_ngram = math.log(self.smooth(ngram))
            else:
                ngram_counts = self.model_dict.get(ngram, 0)
                prefix_counts = self.model_dict_prefix.get(prefix, 0)
                ll_ngram = math.log(ngram_counts / prefix_counts)

            log_likelihood += ll_ngram

        return log_likelihood

    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.
        """
        if self.chars:
            prefix = ''.join(ngram.split()[:-1])
        else:
            prefix = ' '.join(ngram.split()[:-1])

        if self.model_dict.get(ngram):
            ngram_count = self.model_dict[ngram] + 1
            prefix_count = self.model_dict_prefix[prefix] + 1

        elif self.model_dict_prefix.get(prefix):
            ngram_count = 0
            prefix_count = self.model_dict_prefix[prefix] + 1

        else:
            ngram_count = 0
            prefix_count = 0

        ll_ngram = (ngram_count + 1) / (prefix_count + len(self.model_dict_prefix))
        return ll_ngram

def normalize_text(text):  # this is not a class method
    """Returns a normalized version of the specified string.
      The text is normalized based on four corpuses of text (shakespeare, Obama speeches, Trump tweets).

      Args:
        text (str): the text to normalize

      Returns:
        string. the normalized text.
    """
    text = text.lower()
    text = text.replace("*", "")
    use_sos_eos_tokens = False # I've added an option to set sos-eos tokens for better performance
    replacement_dict = {
        r"\n": " ",
        r"-+": "",
        r"http.+?\s": "URL . ",  # normalizing websites URL's (for trump tweets)
        r"\[*<i>\s*\w+?\s*</i>]*": "",  # html patterns replacement (for obama speeches)
        r"\d{1}\"|\"\w{1}": "",   # html patterns replacement (for obama speeches)
        r"<p>": "",  # html patterns replacement (for obama speeches)
        r"@\w+\s|@\w+,|@\w+\.": "@TwitterUsername ",  # normalizing twitter usernames (for trump tweets)
        r"\"": "",
        # r"\"|'": "",
        r"[():]": " , ",
        r";": " . ",  # for obama speeches (decided to convert ; -> . for model simplicity, due to similarity
        r"\\\\\"|\"\\\\|\\\\|\\\s+\\": "",  # for obama speeches
        r"´|…": "",
        r"$\d+": "$SomeMoney",  # normalizing all money mentions to a single amount $SomeMoney
        r"\d{2}.{1}\d{2}.{1}\d{4}": "dd-mm-yyyy",  # normalizing dates
        r"\d{2}.{1}\d{2}": "dd-mm",  # normalizing dates
        r"(1\d{3})|(2\d{3})": "SomeYear",  # normalizing dates
        r"\d+": "SomeNumber",  # normalizing numbers
        r"#\w+": "#Hashtag",  # normalizing twitter hashtags (for trump tweets)
        r"(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )": " ",  # padding after .,!?()
        r"\s*&mdash\s*": " ",  # removing long dash characters
        r"(\s*\.+)+": " . ",  # replacing each multiple '. . .' patterns with single ' . ' for simplicity
        r"\s{2,}": " ",  # removing extra spaces
        r"january|february|march|april|may|june|july|august|september|october|november|december": "SomeMonth",
        r"sunday|monday|tuesday|wednesday|thursday|friday|saturday": "SomeWeekday"
    }

    if use_sos_eos_tokens:
        replacement_dict[r"\s\.\s"] = " <eos> <sos> "

    for k, v in replacement_dict.items():
        text = re.sub(k, v, text)

    return text